{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ettoday 網路爬蟲實作練習\n",
    "\n",
    "\n",
    "* 能夠利用 Request + BeatifulSoup 撰寫爬蟲，並存放到合適的資料結構\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "根據範例：\n",
    "\n",
    "1. 取出今天所有的新聞\n",
    "2. 取出現在時間兩小時內的新聞\n",
    "3. 根據範例，取出三天前下午三點到五點的新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time, datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "browser = webdriver.Chrome(executable_path='../other/chromedriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 取出今天所有的新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/06/29 02:22 畢業後友情消逝！他堅決維繫好累「狂貼冷屁股」　網點破殘酷真相\n",
      "2020/06/29 01:53 「德不配位，撤換陳菊」　蔣萬安：代表人民抗議、佔領議場\n",
      "2020/06/29 01:42 好市多「神秘食物」散發屁味！她咬一口狂喊：太好吃了　網友＋1推爆\n",
      "2020/06/29 01:00 英國「8XL男」3年減133公斤！減肥組織頒發「2020瘦身王」\n",
      "2020/06/29 00:52 藍軍強攻立院之戰會勝利？　名嘴曝「2關鍵時間點」....沒撐過國民黨必亡\n",
      "2020/06/29 00:51 霸佔立院「符合兵法要義」！黃創夏揭2重點：國民黨永遠被看衰\n",
      "2020/06/29 00:39 「民主之恥」支持者包圍立院！警政署長喊嚴辦　羅智強嗆：好膽就來\n",
      "2020/06/29 00:19 一張圖看哪些國民黨立委攻佔立法院　破壞門鎖、噴漆、扔椅子樣樣來\n",
      "2020/06/29 00:18 全智賢、孫藝珍都在背「黑色精品包」　八個爆款LV Pont9、McQueen Story最搶眼\n",
      "2020/06/29 00:16 台南首家五星飯店結束營業！大億麗緻酒店19年劃下句點…黃偉哲到場送別\n",
      "2020/06/29 00:14 竹北市29日「送肉粽」！21:00橋底出發　路線圖曝光\n",
      "2020/06/29 00:09 酸民不想被告狂求饒！　Fin.K.L成員玉珠鉉嗆「你惹錯人了」：我咬了就不會放\n",
      "2020/06/29 00:06 國民黨大動作攻議場　江啟臣恐有3大矛盾、危機\n",
      "2020/06/29 00:03 台南「賭神」家遭開51槍！智慧型武鬥派策劃半年…犯案細節曝光\n"
     ]
    }
   ],
   "source": [
    "browser.get(\"https://www.ettoday.net/news/news-list.htm\")\n",
    "\n",
    "# 取得今天日子 (day)\n",
    "todayDay = datetime.date.today().day;\n",
    "\n",
    "while True:\n",
    "    # 這邊的值有作一些調整, 要不然會有 (1) 捲不動; 或 (2) 當天新聞沒完整出現, 就跳到前一天\n",
    "    time.sleep(3)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight + 10000);\")\n",
    "\n",
    "    html_source = browser.page_source\n",
    "    soup = BeautifulSoup(html_source, \"html5lib\")\n",
    "    \n",
    "    lastNews = soup.find(class_=\"part_list_2\").find_all('h3')[-1]\n",
    "    \n",
    "    # ---- 取得最後一則文章的日子 (Day)\n",
    "    # step 1. assign to string\n",
    "    strDate = lastNews.find(class_=\"date\").text\n",
    "    # step 2. format tranform (string to datetime)\n",
    "    lastNewsDate = datetime.datetime.strptime(strDate,'%Y/%m/%d %H:%M')\n",
    "    # step 3. get day number\n",
    "    lastNewsDay = lastNewsDate.day\n",
    "    \n",
    "    if (lastNewsDay != todayDay):\n",
    "        break\n",
    "\n",
    "    \n",
    "for d in soup.find(class_=\"part_list_2\").find_all('h3'):\n",
    "    # ---- 取得文章的日子 (Day)\n",
    "    strDate = d.find(class_=\"date\").text\n",
    "    newsDate = datetime.datetime.strptime(strDate,'%Y/%m/%d %H:%M')\n",
    "    newsDay = newsDate.day\n",
    "        \n",
    "    if (newsDay == todayDay):\n",
    "        print(d.find(class_=\"date\").text, d.find_all('a')[-1].text)\n",
    "    else:\n",
    "        break      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 取出現在時間兩小時內的新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/06/29 02:22 畢業後友情消逝！他堅決維繫好累「狂貼冷屁股」　網點破殘酷真相\n",
      "2020/06/29 01:53 「德不配位，撤換陳菊」　蔣萬安：代表人民抗議、佔領議場\n",
      "2020/06/29 01:42 好市多「神秘食物」散發屁味！她咬一口狂喊：太好吃了　網友＋1推爆\n",
      "2020/06/29 01:00 英國「8XL男」3年減133公斤！減肥組織頒發「2020瘦身王」\n",
      "2020/06/29 00:52 藍軍強攻立院之戰會勝利？　名嘴曝「2關鍵時間點」....沒撐過國民黨必亡\n",
      "2020/06/29 00:51 霸佔立院「符合兵法要義」！黃創夏揭2重點：國民黨永遠被看衰\n"
     ]
    }
   ],
   "source": [
    "browser.get(\"https://www.ettoday.net/news/news-list.htm\")\n",
    "\n",
    "# 取得現在的小時 (hour)\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "\n",
    "html_source = browser.page_source\n",
    "soup = BeautifulSoup(html_source, \"html5lib\")   \n",
    "\n",
    "while True:\n",
    "    time.sleep(3)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight + 10000);\")\n",
    "        \n",
    "    html_source = browser.page_source\n",
    "    soup = BeautifulSoup(html_source, \"html5lib\")\n",
    "    \n",
    "    # ---- 取得最後一則文章的時間\n",
    "    lastNews = soup.find(class_=\"part_list_2\").find_all('h3')[-1]\n",
    "    strDate = lastNews.find(class_=\"date\").text\n",
    "    lastNewsDate = datetime.datetime.strptime(strDate,'%Y/%m/%d %H:%M')\n",
    "    \n",
    "    # 2個小時內\n",
    "    if (lastNewsDate < now+datetime.timedelta(hours=2)):\n",
    "        break\n",
    "\n",
    "    \n",
    "for d in soup.find(class_=\"part_list_2\").find_all('h3'):\n",
    "    # ---- 取得文章的小時 (Hour)\n",
    "    strDate = d.find(class_=\"date\").text\n",
    "    newsDate = datetime.datetime.strptime(strDate,'%Y/%m/%d %H:%M')\n",
    "    \n",
    "    # 2個小時內\n",
    "    if (newsDate  >= now + datetime.timedelta(hours=-2)):\n",
    "        print(d.find(class_=\"date\").text, d.find_all('a')[-1].text)\n",
    "    else:\n",
    "        break       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 根據範例，取出三天前下午三點到五點的新聞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法一：使用 Request 和 BeautifulSoup ==> 失敗, 無法抓到資料, 因為最早的只能到 1 天半前的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-28 19:24:00\n",
      "2020-06-28 18:57:00\n",
      "2020-06-28 18:37:00\n",
      "2020-06-28 18:18:00\n",
      "2020-06-28 18:02:00\n",
      "2020-06-28 17:56:00\n",
      "2020-06-28 17:40:00\n",
      "2020-06-28 17:19:00\n",
      "2020-06-28 17:05:00\n",
      "2020-06-28 16:53:00\n",
      "2020-06-28 16:47:00\n",
      "2020-06-28 16:36:00\n",
      "2020-06-28 16:22:00\n",
      "2020-06-28 16:07:00\n",
      "2020-06-28 16:00:00\n",
      "2020-06-28 15:45:00\n",
      "2020-06-28 15:21:00\n",
      "2020-06-28 15:03:00\n",
      "2020-06-28 14:43:00\n",
      "2020-06-28 14:28:00\n",
      "2020-06-28 14:04:00\n",
      "2020-06-28 13:45:00\n",
      "2020-06-28 13:31:00\n",
      "2020-06-28 13:21:00\n",
      "2020-06-28 12:51:00\n",
      "2020-06-28 12:24:00\n",
      "2020-06-28 12:09:00\n",
      "2020-06-28 12:02:00\n",
      "2020-06-28 11:48:00\n",
      "2020-06-28 11:34:00\n",
      "2020-06-28 11:10:00\n",
      "2020-06-28 10:58:00\n",
      "2020-06-28 10:41:00\n",
      "2020-06-28 10:24:00\n",
      "2020-06-28 10:16:00\n",
      "2020-06-28 10:03:00\n",
      "2020-06-28 09:50:00\n",
      "2020-06-28 09:17:00\n",
      "2020-06-28 09:00:00\n",
      "2020-06-28 08:20:00\n",
      "2020-06-28 07:35:00\n",
      "2020-06-28 06:00:00\n",
      "2020-06-28 00:56:00\n",
      "2020-06-28 00:06:00\n",
      "2020-06-27 23:16:00\n",
      "2020-06-27 22:00:00\n",
      "2020-06-27 21:26:00\n",
      "2020-06-27 20:49:00\n",
      "2020-06-27 20:25:00\n",
      "2020-06-27 19:38:00\n",
      "2020-06-27 19:03:00\n",
      "2020-06-27 18:19:00\n",
      "2020-06-27 18:06:00\n",
      "2020-06-27 18:00:00\n",
      "2020-06-27 17:46:00\n",
      "2020-06-27 17:33:00\n",
      "2020-06-27 17:23:00\n",
      "2020-06-27 17:10:00\n",
      "2020-06-27 16:57:00\n",
      "2020-06-27 16:39:00\n",
      "2020-06-27 16:28:00\n",
      "2020-06-27 16:07:00\n",
      "2020-06-27 15:59:00\n",
      "2020-06-27 15:37:00\n",
      "2020-06-27 15:15:00\n",
      "2020-06-27 15:02:00\n",
      "2020-06-27 14:35:00\n",
      "2020-06-27 14:21:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n",
      "2020-06-27 14:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0c8f63d838c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mhtml_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html5lib\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# ---- 取得最後一則文章的時間\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[1;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_html5lib.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mextra_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_specified_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;31m# Set the character encoding detected by the tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\html5lib\\html5parser.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \"\"\"\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\html5lib\\html5parser.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(self, stream, innerHTML, container, scripting, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_ReparseException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\html5lib\\html5parser.py\u001b[0m in \u001b[0;36mmainLoop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m                         \u001b[0mnew_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessSpaceCharacters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                     \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mStartTagToken\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m                         \u001b[0mnew_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessStartTag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m                     \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEndTagToken\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m                         \u001b[0mnew_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessEndTag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\html5lib\\html5parser.py\u001b[0m in \u001b[0;36mprocessStartTag\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mprocessStartTag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartTagHandler\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstartTagHtml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\html5lib\\html5parser.py\u001b[0m in \u001b[0;36mstartTagFormatting\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstartTagFormatting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreconstructActiveFormattingElements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddFormattingElement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstartTagNobr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\html5lib\\html5parser.py\u001b[0m in \u001b[0;36maddFormattingElement\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m   1022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatchingElements\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactiveFormattingElements\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatchingElements\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactiveFormattingElements\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \u001b[1;31m# the real deal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "browser.get(\"https://www.ettoday.net/news/news-list.htm\")\n",
    "\n",
    "# 取得三天前的日期 (下午 3點 及 5點)\n",
    "beforeThreeDate = datetime.datetime.now() + timedelta(-3)\n",
    "beforeThreeDayStart = datetime.datetime(beforeThreeDate.year, beforeThreeDate.month, beforeThreeDate.day, 15, 0)\n",
    "beforeThreeDayEnd = datetime.datetime(beforeThreeDate.year, beforeThreeDate.month, beforeThreeDate.day, 17, 0)\n",
    "\n",
    "while True:\n",
    "    time.sleep(3)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight + 10000);\")\n",
    "        \n",
    "    html_source = browser.page_source\n",
    "    soup = BeautifulSoup(html_source, \"html5lib\")\n",
    "    \n",
    "    # ---- 取得最後一則文章的時間\n",
    "    lastNews = soup.find(class_=\"part_list_2\").find_all('h3')[-1]\n",
    "    strDate = lastNews.find(class_=\"date\").text\n",
    "    lastNewsDate = datetime.datetime.strptime(strDate,'%Y/%m/%d %H:%M')\n",
    "    \n",
    "    print(lastNewsDate)\n",
    "    \n",
    "    if (lastNewsDate < beforeThreeDayStart):\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "for d in soup.find(class_=\"part_list_2\").find_all('h3'):\n",
    "    # ---- 取得文章的小時 (Hour)\n",
    "    strDate = d.find(class_=\"date\").text\n",
    "    newsDate = datetime.datetime.strptime(strDate,'%Y/%m/%d %H:%M')\n",
    "    \n",
    "    if (newsDate >= beforeThreeDayStart and newsDate <= beforeThreeDayEnd):\n",
    "        print(d.find(class_=\"date\").text, d.find_all('a')[-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二：API Request + BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/26 17:00 指考點解／英文多用推理、刪去　聖母院、脫歐、同婚、..\n",
      "6/26 17:00 排隊也要吃！南州代天府廟口「2人氣小吃」　金黃臭豆..\n",
      "6/26 17:00 打造五星級衛浴空間！衛生用品怎麼收　「清爽系」收納..\n",
      "6/26 16:59 中職／富邦一軍主戰力調整　洪總要練新人放眼下半季\n",
      "6/26 16:58 西螺名店正莊麻糬　遺孀承襲老闆莊孟釗志願支持消防\n",
      "6/26 16:57 蓬佩奧：裁撤駐德美軍是為應對「中國對印度、東南亞的..\n",
      "6/26 16:56 返工車潮！國5宜蘭到坪林時速40↓　北上恐「塞到晚..\n",
      "6/26 16:55 余香凝清涼泳裝照遭酸「象腿」！ 　淡定反擊：抱歉我..\n",
      "6/26 16:55 日職／連兩戰板凳後　王柏融重回先發左外野\n",
      "6/26 16:53 獨／美魔女丁國琳「直播賣海鮮」求第二春　打入團媽圈..\n",
      "6/26 16:52 快訊／遠雄廣場5歲女童「右腳2腳趾」慘遭手扶梯夾斷..\n",
      "6/26 16:50 吃芒果也要長知識！ 台南大內消防隊拋繡招親創新防溺..\n",
      "6/26 16:50 3跆拳道運動員「夜店強拉正妹」！她男友護愛慘被打死..\n",
      "6/26 16:49 端午連假國道塞爆！他曝「台北開到台東14小時」…無..\n",
      "6/26 16:49 翹臀+六塊肌！日本「猛男筷架」爆紅　網歪樓：筷子放..\n",
      "6/26 16:47 中職／王維中猶豫報名選秀　陳偉殷：多聽多看多想\n",
      "6/26 16:47 緊張狗忘不了收容陰影躲桌底！　入住「網美帳篷」牠獲..\n",
      "6/26 16:42 禁止走路滑手機！大和市通過議案創日本首例　7月1日..\n",
      "6/26 16:40 中職／獅猿打線出爐　勝投王對決施子謙戰霸能\n",
      "6/26 16:40 中職／王威晨中蛋狀況大好轉　賽前正常練打繼續先發\n",
      "6/26 16:40 台大醫院雲林分院火警　院方澄清：冒煙的是負壓隔離「..\n",
      "6/26 16:38 中職／王威晨中彈無礙　胡金龍降二軍林益全扛指定打擊\n",
      "6/26 16:37 保安宮端午節永保平安點燈儀式　獅子會分區主席王伯群..\n",
      "6/26 16:37 8月大男嬰被丟入泳池「學生存」！媽一旁側錄歡呼「你..\n",
      "6/26 16:36 吊車上路故障失控　駕駛避免危險「衝撞無人公車站」...\n",
      "6/26 16:35 淋了芥末美乃滋、柴魚片！肯德基打造全新「大阪燒無骨..\n",
      "6/26 16:34 酒後為友人助陣持鋁棒打死人　莽男遭重判22年還要賠..\n",
      "6/26 16:33 天氣熱無法專心工作　用3方法找回清晰思緒\n",
      "6/26 16:30 日職／陽岱鋼代打敲安　重回先發第5棒對戰養樂多\n",
      "6/26 16:29 旅法甜點鑑賞作家走過徬徨期　揮別不喜歡的領域，找到..\n",
      "6/26 16:28 遠百企業以粽傳愛捐助溪北地區弱勢家庭 黃偉哲期..\n",
      "6/26 16:27 疫情衝擊大聯盟開打　陳偉殷：：交給上天決定\n",
      "6/26 16:25 日女學生疑在台染疫！3研究曝「無症狀者」數量超乎想..\n",
      "6/26 16:23 獨／LISA父親是瑞士名廚！11年前來台演講.....\n",
      "6/26 16:22 「赤崁魁星餅」現蹤　台南古蹟限定神級商品再一發！\n",
      "6/26 16:20 影／蘇巧慧挑戰「盲測吃粽」　靠一粒花生「感應」到蘇..\n",
      "6/26 16:17 台南國際芒果節開跑　消防官稽查青果集貨場為芒果品質..\n",
      "6/26 16:13 長得像塑膠玩具！他在沖繩釣到「噴漆怪魚」...色彩..\n",
      "6/26 16:12 網頁別亂點！端午節首日　高雄無業男險遭詐騙集團控制..\n",
      "6/26 16:11 正妹護士烈日「跪地CPR 20分」...救護車來默..\n",
      "6/26 16:10 惡搞核酸檢測結果　陸男發網：還有救嗎？...公安找..\n",
      "6/26 16:06 姊想生混血娃豪撒447萬！弟赴烏克蘭捐精　喜獲雙胞..\n",
      "6/26 16:05 關於BMW的11個秘密！雙B差點變單B　BMW險被..\n",
      "6/26 16:03 摸我！黑柴躺著啃手「爽到滾下床」裝沒事　媽笑到狂重..\n",
      "6/26 16:02 艾菲爾鐵塔重新開放！關閉104天損失8億元　遊客激..\n",
      "6/26 16:00 50年來最大！撒哈拉「哥吉拉等級」沙塵暴登陸美國　..\n",
      "6/26 16:00 收錄701家在地店家　全台最強台南肉燥攻略地圖「用..\n",
      "6/26 15:59 孫尤安半神隱2年「爆秘婚生子」！　親揭「花200萬..\n",
      "6/26 15:57 《怪奇物語》Eleven天天找他聊男生　亨利卡維爾..\n",
      "6/26 15:57 澳在野黨議員親中遭調查　政府情報單位搜索住家辦公室\n",
      "6/26 15:57 絕對不是近期感染！　莊人祥點出日本女學生確診「2種..\n",
      "6/26 15:57 中職／費爾本提前離隊捨不得　統一獅談妥新洋投\n",
      "6/26 15:56 張清芳斷捨15年婚姻！　黃子佼曝「她行情很高」\n",
      "6/26 15:55 緊身衣太合身...22kg肉感汪「深V爆乳」卡拉鏈..\n",
      "6/26 15:55 買二手房好擔心？「6招」輕鬆鑒定住宅品質...滲水..\n",
      "6/26 15:52 偷吃汪哥零食被抓包！　阿金「愛的抱抱」謝罪融化10..\n",
      "6/26 15:52 走進夢幻海底隧道！澎湖水族館必看3大亮點　鯊魚餵食..\n",
      "6/26 15:49 仙人掌爆長3層樓高！頂部驚見奇葩人形...網全看傻..\n",
      "6/26 15:48 國際級高峰會活動5天！一張門票看到飽　每天24hr..\n",
      "6/26 15:46 燒烤店遇「毛毛店小二」幫點餐！　老闆曝：點牛排就有..\n",
      "6/26 15:44 香港記協致信聯合國　要求成立小組調查港警打壓傳媒\n",
      "6/26 15:44 訂獨棟民宿！隔壁辦喪事「一家全額付款」退房　酸民神..\n",
      "6/26 15:44 影／坦言北韓最近很奇怪...日防衛相分析3可能：金..\n",
      "6/26 15:42 墨國三胞胎「出生4hr即確診」！爸媽檢測皆陰性　醫..\n",
      "6/26 15:41 吃膩端午粽子！她曝光「阿嬤冷門食譜」無敵簡單　軟糯..\n",
      "6/26 15:40 打掉陽台「睡在樑柱下」　她讚空間大...長輩驚勸：..\n",
      "6/26 15:39 中職／洪總看富邦氣氛還可以　李宗賢扛開路先鋒\n",
      "6/26 15:39 周曉涵「妹妹頭再現」新髮型超仙！　35歲真實長相現..\n",
      "6/26 15:37 劈腿妹懷孕不知孩子爸是誰　「一個結婚一個當ATM」..\n",
      "6/26 15:36 曼聯球星為喬帥叫屈　科里奇聲稱感染至今無症狀\n",
      "6/26 15:32 「嬰兒車」掛重物噴飛幼娃！家長粗心行徑釀禍　急診醫..\n",
      "6/26 15:30 錄影吃豆腐偷聞「女神級嘉賓」腳底！　本尊正面曝光…..\n",
      "6/26 15:25 美軍偵察機早上偵蒐中午共機就來！國防部：進入我西南..\n",
      "6/26 15:25 70年後終於能回家！6架戰機護送韓戰軍人147具遺..\n",
      "6/26 15:22 外傳當選後韓國瑜舊團隊將「復辟」　李眉蓁：請不要戴..\n",
      "6/26 15:21 清爽潔顏必備！美妝版好評　最值得入手的10款經典卸..\n",
      "6/26 15:20 桃園市免費樂活巴士7/15恢復正常班次　減班48條..\n",
      "6/26 15:19 快訊／台大醫院斗六分院「空調冒煙」！設備燒到焦黑　..\n",
      "6/26 15:18 中職／富邦戰力調整　倪福德、王詩聰等4人上一軍\n",
      "6/26 15:15 中職／富邦胡金龍群組嗆聲　二度發聲明強調非本人\n",
      "6/26 15:15 余文樂破氣球「揭曉二胎性別」　一見結果後仰大叫…全..\n",
      "6/26 15:15 感謝卡特提攜　林書豪：你是無與倫比的良師益友\n",
      "6/26 15:14 印度反華加劇！　德里3000家旅館「拒絕接待中國人..\n",
      "6/26 15:10 快訊／雨彈來襲！全台17縣市豪大雨特報　北北基「大..\n",
      "6/26 15:06 17縣市豪大雨特報！台北市信義南港、新北新莊汐止「..\n",
      "6/26 15:05 中職／捲入茶壺風暴胡金龍道歉！　強調自己責無旁貸\n",
      "6/26 15:03 冷氣抖出一堆「動物大便」　爺爺當天高燒39度「疑染..\n",
      "6/26 15:00 眺望藍海＋浪漫夕陽！馬祖超美海景蒙古包　餐酒館有藍..\n",
      "6/26 15:00 家有籃球場、運動館！宅在家也可以很健康　台中瘋推「..\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# 取得三天前的日期 (下午 3點 及 5點)\n",
    "beforeThreeDate = datetime.datetime.now() + timedelta(-3)\n",
    "beforeThreeDayStart = datetime.datetime(beforeThreeDate.year, beforeThreeDate.month, beforeThreeDate.day, 15, 0)\n",
    "beforeThreeDayEnd = datetime.datetime(beforeThreeDate.year, beforeThreeDate.month, beforeThreeDate.day, 17, 0)\n",
    "\n",
    "def requestHtml(offset):\n",
    "    url = 'https://www.ettoday.net/show_roll.php'\n",
    "    filename = str(beforeThreeDate.year)+str(beforeThreeDate.month).zfill(2)+str(beforeThreeDate.day).zfill(2)\n",
    "    myData = {'tFile':filename+'.xml', 'tPage':1, 'offset':offset}\n",
    "    r = requests.post(url, data = myData)\n",
    "    r = r.text.replace('\\t','').replace('\\n','').replace('  ','')\n",
    "    return r\n",
    "\n",
    "def printNews(soup):\n",
    "    for d in soup.find_all('div'):\n",
    "        # ---- 取得文章的小時 (Hour)\n",
    "        strDate = d.find(class_=\"date\").text\n",
    "        newsDate = datetime.datetime.strptime(str(beforeThreeDate.year)+'/'+strDate,'%Y/%m/%d %H:%M')\n",
    "\n",
    "        if (newsDate >= beforeThreeDayStart and newsDate <= beforeThreeDayEnd):\n",
    "            print(d.find(class_=\"date\").text, d.find_all('a')[-1].text)\n",
    "\n",
    "totalNews = ''\n",
    "offset = 1\n",
    "\n",
    "while True:\n",
    "    r = requestHtml(offset)\n",
    "    if (r==''):\n",
    "        break\n",
    "    else:\n",
    "        soup = BeautifulSoup(r, \"html5lib\")\n",
    "        offset = offset + 1\n",
    "        \n",
    "        # ---- 取得最後一則文章的時間\n",
    "        lastNews = soup.find_all('div')[-1]\n",
    "        strDate = lastNews.find(class_=\"date\").text\n",
    "        lastNewsDate = datetime.datetime.strptime(str(beforeThreeDate.year)+'/'+strDate,'%Y/%m/%d %H:%M')\n",
    "        printNews(soup)\n",
    "        \n",
    "        if (lastNewsDate < beforeThreeDayStart):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
